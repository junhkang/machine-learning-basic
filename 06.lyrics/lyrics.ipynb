{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7778abe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import glob  \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as svm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tensorflow.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82541d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Busted flat in Baton Rouge, waitin' for a train\", \"And I's feelin' near as faded as my jeans\", 'Bobby thumbed a diesel down, just before it rained']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re \n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*' \n",
    "\n",
    "txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e67c3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "819b7caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> busted flat in baton rouge , waitin for a train <end>',\n",
       " '<start> and i s feelin near as faded as my jeans <end>',\n",
       " '<start> bobby thumbed a diesel down , just before it rained <end>',\n",
       " '<start> it rode us all the way to new orleans i pulled my harpoon out of my dirty red bandanna <end>',\n",
       " '<start> i was playin soft while bobby sang the blues , yeah <end>',\n",
       " '<start> windshield wipers slappin time , i was holdin bobby s hand in mine <end>',\n",
       " '<start> we sang every song that driver knew freedom s just another word for nothin left to lose <end>',\n",
       " '<start> nothin , don t mean nothin hon if it ain t free , no no <end>',\n",
       " '<start> and , feelin good was easy , lord , when he sang the blues <end>',\n",
       " '<start> you know , feelin good was good enough for me <end>',\n",
       " '<start> good enough for me and my bobby mcghee from the kentucky coal mine to the california sun <end>',\n",
       " '<start> there bobby shared the secrets of my soul <end>',\n",
       " '<start> through all kinds of weather , through everything we done <end>',\n",
       " '<start> yeah , bobby baby kept me from the cold one day up near salinas , lord , i let him slip away <end>',\n",
       " '<start> he s lookin for that home , and i hope he finds it <end>',\n",
       " '<start> but , i d trade all of my tomorrows , for a single yesterday <end>',\n",
       " '<start> to be holdin bobby s body next to mine freedom s just another word for nothin left to lose <end>',\n",
       " '<start> nothin , that s all that bobby left me , yeah <end>',\n",
       " '<start> but , feelin good was easy , lord , when he sang the blues <end>',\n",
       " '<start> hey , feelin good was good enough for me , mm hmm <end>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    #if len(sentence) == 0: continue\n",
    "    #if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    #if len(preprocessed_sentence)<=15 & len(preprocessed_sentence)>0:\n",
    "        #print(len(preprocessed_sentence))\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb2afcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    # tokenizer.fit_on_texts(texts): 문자 데이터를 입력받아 리스트의 형태로 변환하는 메서드\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    # tokenizer.texts_to_sequences(texts): 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환하는 메서드\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=20)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7e0d550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 3609 1692 ...    0    0    0]\n",
      " [   2    8    5 ...    0    0    0]\n",
      " [   2  804 7664 ...    0    0    0]\n",
      " ...\n",
      " [   2    5   22 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f38241e5340>\n"
     ]
    }
   ],
   "source": [
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ccccaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 3609 1692 ...    0    0    0]\n",
      " [   2    8    5 ...    0    0    0]\n",
      " [   2  804 7664 ...    0    0    0]\n",
      " ...\n",
      " [   2    5   22 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42767a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 19), (256, 19)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]   \n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    " # tokenizer.num_words: 주어진 데이터의 문장들에서 빈도수가 높은 n개의 단어만 선택\n",
    " # tokenize() 함수에서 num_words를 7000개로 선언했기 때문에, tokenizer.num_words의 값은 12000\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc8f3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = tensor[:len(tensor)]\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(tensor,\n",
    "                                                     y,\n",
    "                                                     test_size=0.2,\n",
    "                                                     random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d580d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다.\n",
    "        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
    "        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다. \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "# embedding size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n",
    "# 그만큼 충분한 데이터가 없으면 안좋은 결과 값을 가져옵니다!   \n",
    "embedding_size = 256 # 워드 벡터의 차원수를 말하며 단어가 추상적으로 표현되는 크기입니다.\n",
    "hidden_size = 1024 # 모델에 얼마나 많은 일꾼을 둘 것인가? 정도로 이해하면 좋다.\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) # tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "552550ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 19, 12001), dtype=float32, numpy=\n",
       "array([[[ 8.75449769e-05,  2.09681835e-04,  3.93544582e-07, ...,\n",
       "         -2.06197248e-04, -4.82125433e-05, -3.87434207e-04],\n",
       "        [ 2.93364370e-04,  6.87636275e-05,  4.19411896e-04, ...,\n",
       "         -6.05747511e-04, -4.68874816e-04, -3.08196148e-04],\n",
       "        [ 6.29507180e-04,  2.02854062e-05,  6.92403351e-04, ...,\n",
       "         -8.15634790e-04, -8.35248211e-04, -3.83240811e-04],\n",
       "        ...,\n",
       "        [ 4.15055460e-04,  1.33915944e-03,  4.00634017e-04, ...,\n",
       "          1.70900690e-04,  7.11548724e-04,  9.17389174e-04],\n",
       "        [ 1.89848768e-04,  1.80041441e-03,  2.52906088e-04, ...,\n",
       "          4.66079131e-04,  1.01077079e-03,  7.61409581e-04],\n",
       "        [-2.10941289e-05,  2.25443067e-03,  1.05923238e-04, ...,\n",
       "          7.01584795e-04,  1.25851633e-03,  6.40758954e-04]],\n",
       "\n",
       "       [[ 8.75449769e-05,  2.09681835e-04,  3.93544582e-07, ...,\n",
       "         -2.06197248e-04, -4.82125433e-05, -3.87434207e-04],\n",
       "        [ 2.40776330e-06,  2.44268827e-04, -6.39657810e-05, ...,\n",
       "         -4.26756334e-04,  4.29763895e-05, -3.07062699e-04],\n",
       "        [-8.26659962e-05,  2.99854786e-04, -1.22480778e-04, ...,\n",
       "         -3.47607158e-04, -2.85998889e-04, -1.76836067e-04],\n",
       "        ...,\n",
       "        [-1.00728334e-03,  3.47929448e-03, -2.29933110e-04, ...,\n",
       "          1.47560646e-03,  1.81060995e-03, -2.98377359e-04],\n",
       "        [-1.03654910e-03,  3.77361174e-03, -3.17942322e-04, ...,\n",
       "          1.48214900e-03,  1.87949953e-03, -2.43284288e-04],\n",
       "        [-1.05142791e-03,  4.03739186e-03, -4.04498249e-04, ...,\n",
       "          1.47333392e-03,  1.93290948e-03, -1.69617822e-04]],\n",
       "\n",
       "       [[ 8.75449769e-05,  2.09681835e-04,  3.93544582e-07, ...,\n",
       "         -2.06197248e-04, -4.82125433e-05, -3.87434207e-04],\n",
       "        [-1.65966092e-04,  1.28253363e-04,  4.01540638e-05, ...,\n",
       "         -2.95208913e-04, -1.59947886e-04, -5.19881258e-04],\n",
       "        [-3.51267518e-04,  2.66148592e-04,  6.08220835e-05, ...,\n",
       "         -7.03945581e-04, -2.78757128e-04, -4.38429881e-04],\n",
       "        ...,\n",
       "        [-9.80090001e-04,  3.20259039e-03, -3.10352916e-04, ...,\n",
       "          1.18742906e-03,  1.91089057e-03, -6.81934936e-04],\n",
       "        [-1.04643800e-03,  3.52591812e-03, -3.87829670e-04, ...,\n",
       "          1.27984770e-03,  1.98692689e-03, -5.76277787e-04],\n",
       "        [-1.08612585e-03,  3.81964445e-03, -4.61598043e-04, ...,\n",
       "          1.33952871e-03,  2.04331870e-03, -4.58640861e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 8.75449769e-05,  2.09681835e-04,  3.93544582e-07, ...,\n",
       "         -2.06197248e-04, -4.82125433e-05, -3.87434207e-04],\n",
       "        [ 3.61589773e-04,  2.64948845e-04, -1.85258556e-04, ...,\n",
       "         -2.13118241e-04,  1.53881439e-04, -2.26952412e-04],\n",
       "        [ 4.31953173e-04,  3.73875839e-04, -2.26413409e-04, ...,\n",
       "         -1.42120834e-05,  4.27563122e-04, -2.43921313e-04],\n",
       "        ...,\n",
       "        [-1.05956511e-03,  4.67263255e-03, -6.63931947e-04, ...,\n",
       "          1.36883417e-03,  1.96977681e-03,  1.87967773e-04],\n",
       "        [-1.04203459e-03,  4.83330525e-03, -7.11209781e-04, ...,\n",
       "          1.34237786e-03,  1.99489924e-03,  2.79403583e-04],\n",
       "        [-1.01724942e-03,  4.97120153e-03, -7.54104462e-04, ...,\n",
       "          1.31655112e-03,  2.01504235e-03,  3.64647654e-04]],\n",
       "\n",
       "       [[ 8.75449769e-05,  2.09681835e-04,  3.93544582e-07, ...,\n",
       "         -2.06197248e-04, -4.82125433e-05, -3.87434207e-04],\n",
       "        [ 1.84783887e-04,  2.62212503e-04,  1.02501195e-04, ...,\n",
       "         -2.48205004e-04,  2.11939168e-05, -7.43989251e-04],\n",
       "        [ 1.95169661e-04,  2.31891667e-04,  1.36602866e-05, ...,\n",
       "         -1.51144137e-04, -4.85689816e-04, -9.79470089e-04],\n",
       "        ...,\n",
       "        [ 3.30107832e-05,  2.08629016e-03, -7.87370955e-04, ...,\n",
       "          2.12016655e-03,  1.37620675e-03, -3.52601055e-04],\n",
       "        [-1.54405599e-04,  2.48729228e-03, -7.10023043e-04, ...,\n",
       "          2.10479531e-03,  1.54464447e-03, -3.73068469e-04],\n",
       "        [-3.28257534e-04,  2.86559621e-03, -6.57805940e-04, ...,\n",
       "          2.06450350e-03,  1.67343544e-03, -3.58407939e-04]],\n",
       "\n",
       "       [[ 8.75449769e-05,  2.09681835e-04,  3.93544582e-07, ...,\n",
       "         -2.06197248e-04, -4.82125433e-05, -3.87434207e-04],\n",
       "        [-4.21065721e-04,  2.18677742e-04, -8.80771586e-06, ...,\n",
       "         -2.51160644e-04, -8.59715801e-05, -5.74130856e-04],\n",
       "        [-3.87450476e-04,  9.87313033e-05, -1.32688539e-04, ...,\n",
       "         -2.61178619e-04, -2.32408449e-04, -5.47728909e-04],\n",
       "        ...,\n",
       "        [ 6.27904374e-05, -7.72939471e-04,  5.98466897e-04, ...,\n",
       "         -7.33096067e-06,  9.30917508e-04, -1.72593130e-03],\n",
       "        [ 1.64432306e-04, -4.69889259e-04,  4.62544645e-04, ...,\n",
       "          2.34457329e-04,  1.32954610e-03, -1.56428479e-03],\n",
       "        [ 1.38277217e-04, -8.58127678e-05,  3.58524034e-04, ...,\n",
       "          4.83728218e-04,  1.66404911e-03, -1.48296449e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02e3d034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5961074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "730/730 [==============================] - 168s 226ms/step - loss: 2.6691\n",
      "Epoch 2/10\n",
      "730/730 [==============================] - 173s 237ms/step - loss: 2.3005\n",
      "Epoch 3/10\n",
      "730/730 [==============================] - 173s 237ms/step - loss: 2.1688\n",
      "Epoch 4/10\n",
      "730/730 [==============================] - 173s 237ms/step - loss: 2.0679\n",
      "Epoch 5/10\n",
      "730/730 [==============================] - 174s 238ms/step - loss: 1.9833\n",
      "Epoch 6/10\n",
      "730/730 [==============================] - 174s 237ms/step - loss: 1.9042\n",
      "Epoch 7/10\n",
      "730/730 [==============================] - 174s 237ms/step - loss: 1.8319\n",
      "Epoch 8/10\n",
      "730/730 [==============================] - 173s 237ms/step - loss: 1.7656\n",
      "Epoch 9/10\n",
      "730/730 [==============================] - 174s 238ms/step - loss: 1.7031\n",
      "Epoch 10/10\n",
      "730/730 [==============================] - 174s 238ms/step - loss: 1.6437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f380c06d8e0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam() # Adam은 현재 가장 많이 사용하는 옵티마이저이다. 자세한 내용은 차차 배운다.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, \n",
    "    reduction='none'  \n",
    ")\n",
    "model.compile(loss=loss, optimizer=optimizer) \n",
    "model.fit(dataset, epochs=10) # 만들어둔 데이터셋으로 모델을 학습한다. 30번 학습을 반복하겠다는 의미다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c5170ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) \n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성성\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4 \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0451f2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m not gonna crack <end> '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952f500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
